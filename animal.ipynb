{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9alOWDjTjFlgZN6+AIP2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cocolian/cocolian-nlp/blob/master/animal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "T03JSwAQDiam",
        "outputId": "e107a7b3-632b-4a6b-fbf2-f093b6ad23a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1d56fa3d8f56>\u001b[0m in \u001b[0;36m<cell line: 217>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0minst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnimals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_vgg_ext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-1d56fa3d8f56>\u001b[0m in \u001b[0;36mrun_vgg_ext_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_vgg_ext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_vgg_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         data_augmentation = keras.Sequential(\n",
            "\u001b[0;32m<ipython-input-1-1d56fa3d8f56>\u001b[0m in \u001b[0;36mprepare_vgg_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_vgg_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         self.conv_base = keras.applications.vgg16.VGG16(\n\u001b[1;32m    125\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"imagenet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-1d56fa3d8f56>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/lixiongfeng5/projects/mnielsen/data/animals\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         self.train_dataset = tf.keras.utils.image_dataset_from_directory(\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mdata_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inferred\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/image_dataset.py\u001b[0m in \u001b[0;36mimage_dataset_from_directory\u001b[0;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     image_paths, labels, class_names = dataset_utils.index_directory(\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/dataset_utils.py\u001b[0m in \u001b[0;36mindex_directory\u001b[0;34m(directory, labels, formats, class_names, shuffle, seed, follow_links)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0msubdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mlist_directory_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    766\u001b[0m   \"\"\"\n\u001b[1;32m    767\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     raise errors.NotFoundError(\n\u001b[0m\u001b[1;32m    769\u001b[0m         \u001b[0mnode_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Could not find directory /Users/lixiongfeng5/projects/mnielsen/data/animals/train"
          ]
        }
      ],
      "source": [
        "import glob, os, shutil, pathlib\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Animals:\n",
        "    def move_files(self, src_dir, dest_dir, pattern):\n",
        "        dest = pathlib.PurePath(dest_dir, pattern)\n",
        "        os.makedirs(dest, exist_ok=True)\n",
        "        for filename in glob.glob(src_dir + '/' + pattern + '*'):\n",
        "            shutil.copy(filename, dest)\n",
        "            # print(filename)\n",
        "\n",
        "    def make_subset(self, subset, from_index, end_index):\n",
        "        src = pathlib.Path(\"/Users/lixiongfeng5/projects/mnielsen/data/animals\")\n",
        "        base = pathlib.Path(src, subset)\n",
        "        for cat in (\"dog\", \"cat\"):\n",
        "            dest = pathlib.Path(base, cat)\n",
        "            os.makedirs(dest, exist_ok=True)\n",
        "            print(\"from {src}  to {dest}\")\n",
        "            for fname in [f\"{cat}.{index}.jpg\" for index in range(from_index, end_index)]:\n",
        "                shutil.move(src=src / cat / fname, dst=dest)\n",
        "\n",
        "    def prepare_images(self):\n",
        "        src = \"/Users/lixiongfeng5/projects/mnielsen/data/dogs-vs-cats/\"\n",
        "        dest = \"/Users/lixiongfeng5/projects/mnielsen/data/animals\"\n",
        "        self.prepare_data(src, dest, \"dog\")\n",
        "        self.prepare_data(src, dest, \"cat\")\n",
        "\n",
        "    def prepare_data(self):\n",
        "        data_dir = pathlib.Path(\"/Users/lixiongfeng5/projects/mnielsen/data/animals\")\n",
        "\n",
        "        self.train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            data_dir / \"train\",\n",
        "            labels=\"inferred\",\n",
        "            image_size=(180, 180),\n",
        "            batch_size=32)\n",
        "        print(\"train_dataset size : \", self.train_dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "        self.train_dataset = self.train_dataset.take(50)\n",
        "        print(\"token train_dataset size : \", self.train_dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "        self.test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            data_dir / \"test\",\n",
        "            labels=\"inferred\",\n",
        "            image_size=(180, 180),\n",
        "            batch_size=32)\n",
        "        print(\"test_dataset size : \", self.test_dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "        self.test_dataset = self.test_dataset.take(25)\n",
        "        print(\"token test_dataset size : \", self.test_dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "        self.validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            data_dir / \"validation\",\n",
        "            labels=\"inferred\",\n",
        "            image_size=(180, 180),\n",
        "            batch_size=32)\n",
        "        print(\"validation_dataset size : \", self.test_dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "        self.validation_dataset = self.validation_dataset.take(50)\n",
        "        print(\"token validation_dataset size : \", self.test_dataset.reduce(0, lambda x,_: x+1).numpy())\n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        inputs = keras.Input(shape=(180, 180, 3))  ## ←----模型输入应该是尺寸为180×180的RGB图像\n",
        "        data_augmentation = keras.Sequential(\n",
        "            [\n",
        "                layers.RandomFlip(\"horizontal\"),\n",
        "                layers.RandomRotation(0.1),\n",
        "                layers.RandomZoom(0.2),\n",
        "            ]\n",
        "        )\n",
        "        x = data_augmentation(inputs)\n",
        "        x = layers.Rescaling(1. / 255)(x)  ##←----将输入除以255，使其缩放至[0, 1]区间\n",
        "        x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "        x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "        x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "        x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "        x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        model.compile(loss=\"binary_crossentropy\",\n",
        "                      optimizer=\"rmsprop\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def plot_result(self, history):\n",
        "        accuracy = history.history[\"accuracy\"]\n",
        "        val_accuracy = history.history[\"val_accuracy\"]\n",
        "        loss = history.history[\"loss\"]\n",
        "        val_loss = history.history[\"val_loss\"]\n",
        "        epochs = range(1, len(accuracy) + 1)\n",
        "        plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "        plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "        plt.title(\"Training and validation accuracy\")\n",
        "        plt.legend()\n",
        "        plt.figure()\n",
        "        plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "        plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "        plt.title(\"Training and validation loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    def get_features_and_labels(self, conv_base, dataset):\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        for images, labels in dataset:\n",
        "            preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "            features = conv_base.predict(preprocessed_images)\n",
        "            all_features.append(features)\n",
        "            all_labels.append(labels)\n",
        "        return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "    def prepare_vgg_data(self):\n",
        "        self.prepare_data()\n",
        "        self.conv_base = keras.applications.vgg16.VGG16(\n",
        "            weights=\"imagenet\",\n",
        "            include_top=False,\n",
        "            input_shape=(180, 180, 3))\n",
        "        self.train_features, self.train_labels = self.get_features_and_labels(self.conv_base, self.train_dataset)\n",
        "        self.val_features, self.val_labels = self.get_features_and_labels(self.conv_base, self.validation_dataset)\n",
        "        self.test_features, self.test_labels = self.get_features_and_labels(self.conv_base, self.test_dataset)\n",
        "\n",
        "\n",
        "\n",
        "    def run_vgg16(self):\n",
        "\n",
        "        self.prepare_vgg_data()\n",
        "        inputs = keras.Input(shape=(5, 5, 512))\n",
        "        x = layers.Flatten()(inputs)  ## ←----请注意，将特征传入Dense层之前，需要先经过Flatten层\n",
        "        x = layers.Dense(256)(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "\n",
        "        model.compile(loss=\"binary_crossentropy\",\n",
        "                      optimizer=\"rmsprop\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "        callbacks = [\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                filepath=\"feature_extraction.kr\",\n",
        "                save_best_only=True,\n",
        "                monitor=\"val_loss\")\n",
        "        ]\n",
        "        history = model.fit(\n",
        "            self.train_features, self.train_labels,\n",
        "            epochs=20,\n",
        "            validation_data=(self.val_features, self.val_labels),\n",
        "            callbacks=callbacks)\n",
        "        self.plot_result(history)\n",
        "\n",
        "    def run_vgg_ext_model(self):\n",
        "        self.prepare_vgg_data()\n",
        "\n",
        "        data_augmentation = keras.Sequential(\n",
        "            [\n",
        "                layers.RandomFlip(\"horizontal\"),\n",
        "                layers.RandomRotation(0.1),\n",
        "                layers.RandomZoom(0.2),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        inputs = keras.Input(shape=(180, 180, 3))\n",
        "        x = data_augmentation(inputs)  ##←----使用数据增强\n",
        "        x = keras.applications.vgg16.preprocess_input(x)  ##←----对输入值进行缩放\n",
        "        x = self.conv_base(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(256)(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        model = keras.Model(inputs, outputs)\n",
        "        model.compile(loss=\"binary_crossentropy\",\n",
        "                      optimizer=\"rmsprop\",\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "        callbacks = [\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                filepath=\"feature_extraction_with_data_augmentation.kr\",\n",
        "                save_best_only=True,\n",
        "                monitor=\"val_loss\")\n",
        "        ]\n",
        "        history = model.fit(\n",
        "            self.train_dataset,\n",
        "            epochs=50,\n",
        "            validation_data=self.validation_dataset,\n",
        "            callbacks=callbacks)\n",
        "\n",
        "        self.plot_result(history)\n",
        "\n",
        "    def run_model(self):\n",
        "        self.prepare_data()\n",
        "        model = self.build_model()\n",
        "        model.summary()\n",
        "        callbacks = [\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                filepath=\"convnet_from_scratch_with_augmentation.log\",\n",
        "                save_best_only=True,\n",
        "                monitor=\"val_loss\")\n",
        "        ]\n",
        "        history = model.fit(\n",
        "            self.train_dataset,\n",
        "            epochs=30,\n",
        "            validation_data=self.validation_dataset,\n",
        "            callbacks=callbacks)\n",
        "        self.plot_result(history)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    inst = Animals()\n",
        "    inst.run_vgg_ext_model()"
      ]
    }
  ]
}